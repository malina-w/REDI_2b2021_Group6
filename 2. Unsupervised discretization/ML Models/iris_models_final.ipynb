{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "514e58db",
   "metadata": {},
   "source": [
    "# Classification models\n",
    "\n",
    "Dataset: iris\n",
    "By: Sam\n",
    "Update at: 28/06/2022\n",
    "\n",
    "====\n",
    "\n",
    "Summary:<br>\n",
    "- Import unsupervised discretised datasets (already encoded categorical attributes)\n",
    "- Split dataset: 75% training, 25% testing, seed = 30\n",
    "- Perform 3 classification models: ID3, Naive Bayes, Knn-VDM\n",
    "- Cross validation (accuracy): 10 folds, repeats: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e959660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from pandas import set_option\n",
    "import numpy as np\n",
    "from numpy import arange\n",
    "## EDA\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c39986bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "# Cross validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score # 1 metric\n",
    "from sklearn.model_selection import cross_validate # more than 1 metric\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76585941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RIPPER (https://pypi.org/project/wittgenstein/) Only for binary\n",
    "import wittgenstein as lw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1d3237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Naive Bayes\n",
    "from sklearn.naive_bayes import CategoricalNB # Categorical Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB # Multinominal Naive Bayes (suitable for NLP)\n",
    "from mixed_naive_bayes import MixedNB # Mixed Naive Bayes for combination of both discrete & continuous feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e9ddea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For decision tree ID3 \n",
    "# https://stackoverflow.com/questions/61867945/python-import-error-cannot-import-name-six-from-sklearn-externals\n",
    "import six\n",
    "import sys\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "import mlrose\n",
    "from id3 import Id3Estimator # ID3 Decision Tree (https://pypi.org/project/decision-tree-id3/)\n",
    "from id3 import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "beb75f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knn-VDM 3\n",
    "from vdm3 import ValueDifferenceMetric\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "039c536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For model evaluation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4ec1b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13679f21",
   "metadata": {},
   "source": [
    "# 1. EWD data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f773ca",
   "metadata": {},
   "source": [
    "## 1.1 EWD, k = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c5bf499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype\n",
      "---  ------   --------------  -----\n",
      " 0   slength  150 non-null    int64\n",
      " 1   swidth   150 non-null    int64\n",
      " 2   plength  150 non-null    int64\n",
      " 3   pwidth   150 non-null    int64\n",
      " 4   label    150 non-null    int64\n",
      "dtypes: int64(5)\n",
      "memory usage: 6.0 KB\n",
      "(150, 4) (150,)\n",
      "Class representation - original:  Counter({0: 50, 1: 50, 2: 50})\n",
      "Class representation - training data:  Counter({1: 39, 0: 38, 2: 35})\n",
      "Class representation - testing data:  Counter({2: 15, 0: 12, 1: 11})\n"
     ]
    }
   ],
   "source": [
    "# Complete code for data preperation\n",
    "# Read data\n",
    "df_ewd1 = pd.read_csv('iris_ewd1.csv')\n",
    "disc = 'EWD'\n",
    "k = 4\n",
    "\n",
    "df_ewd1.info()\n",
    "data = df_ewd1.values\n",
    "data.shape\n",
    "\n",
    "features = df_ewd1.drop('label', axis = 1).columns\n",
    "\n",
    "# separate the data into X and y\n",
    "X = data[:, : len(features)]\n",
    "Y = data[:,-1]\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "# Split train test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state = 30)\n",
    "\n",
    "# Check representation of class\n",
    "print('Class representation - original: ', Counter(Y)) \n",
    "print('Class representation - training data: ', Counter(y_train)) \n",
    "print('Class representation - testing data: ', Counter(y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22930726",
   "metadata": {},
   "source": [
    "### Models - EWD, k=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90130447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       0.65      1.00      0.79        11\n",
      "           2       1.00      0.60      0.75        15\n",
      "\n",
      "    accuracy                           0.84        38\n",
      "   macro avg       0.88      0.87      0.85        38\n",
      "weighted avg       0.90      0.84      0.84        38\n",
      "\n",
      "Time for training model ID3 - default, EWD, k = 4 is: 0.008698225021362305.\n"
     ]
    }
   ],
   "source": [
    "# ID3 - Default\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "\n",
    "model_id3 = Id3Estimator()\n",
    "model_id3.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_id3 = model_id3.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_id3))\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time for training model ID3 - default, {disc}, k = {k} is: {end - start}.') # Total time execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b78ab03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       0.73      1.00      0.85        11\n",
      "           2       1.00      0.73      0.85        15\n",
      "\n",
      "    accuracy                           0.89        38\n",
      "   macro avg       0.91      0.91      0.90        38\n",
      "weighted avg       0.92      0.89      0.89        38\n",
      "\n",
      "Time for training model Naive Bayes - default, EWD, k = 4 is: 0.007327079772949219.\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes - Default\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "model_nb = CategoricalNB()\n",
    "model_nb.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_nb = model_nb.predict(x_test)\n",
    "model_nb.classes_\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "end = time.time()\n",
    "print(f'Time for training model Naive Bayes - default, {disc}, k = {k} is: {end - start}.') # Total time execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a0ca119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       0.71      0.91      0.80        11\n",
      "           2       0.92      0.73      0.81        15\n",
      "\n",
      "    accuracy                           0.87        38\n",
      "   macro avg       0.88      0.88      0.87        38\n",
      "weighted avg       0.88      0.87      0.87        38\n",
      "\n",
      "Time for training model Knn-VDM, EWD, k = 4 is: 0.8173389434814453.\n"
     ]
    }
   ],
   "source": [
    "# Knn-VDM complete code\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "\n",
    "# specific the continuous columns index if any\n",
    "vdm = ValueDifferenceMetric(x_train, y_train, continuous = None)\n",
    "vdm.fit()\n",
    "# Knn model, n_neigbour = 3, metrics = vdm\n",
    "knn_vdm = KNeighborsClassifier(n_neighbors=3, metric=vdm.get_distance, algorithm='brute')\n",
    "## Fit model\n",
    "knn_vdm.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_knn = knn_vdm.predict(x_test)\n",
    "knn_vdm.classes_\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time for training model Knn-VDM, {disc}, k = {k} is: {end - start}.') # Total time execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139d33b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CROSS VALIDATION\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# param\n",
    "num_folds = 10\n",
    "num_repeat = 3\n",
    "seed = 7\n",
    "scores = 'accuracy'\n",
    "\n",
    "print(f'Cross validation result, {scores}, {disc}, k = {k}.')\n",
    "\n",
    "# Create list of algorithms\n",
    "models = []\n",
    "models.append(('ID3', Id3Estimator()))\n",
    "#models.append(('RIPPER', lw.RIPPER()))\n",
    "models.append(('CNB', CategoricalNB()))\n",
    "models.append(('Knn-VDM', KNeighborsClassifier(n_neighbors=3, metric=vdm.get_distance, algorithm='brute')))\n",
    "\n",
    "# Evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "  #kfold = KFold(n_splits=num_folds, shuffle = True, random_state=10)\n",
    "    kfold = RepeatedKFold(n_splits=num_folds, n_repeats=num_repeat, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scores)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = '%s: - Mean: %f, Standard deviation: %f' % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac5b251",
   "metadata": {},
   "source": [
    "### Evaluation, EDW, k = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8710ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "#This library is used to decompose bias and variance in our models\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8328ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 0.174\n",
      "Average bias: 0.158\n",
      "Average variance: 0.055\n",
      "Sklearn 0-1 loss: 0.158\n"
     ]
    }
   ],
   "source": [
    "# ID3\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "model_id3, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_id3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "71a974db",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 6 is out of bounds for axis 1 with size 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-98c83e9ff00f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Naive Bayes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel_nb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'0-1_loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m random_seed=123)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlxtend/evaluate/bias_variance_decomp.py\u001b[0m in \u001b[0;36mbias_variance_decomp\u001b[0;34m(estimator, X_train, y_train, X_test, y_test, loss, num_rounds, random_seed, **fit_params)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_boot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_boot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mall_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mjll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1459\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1461\u001b[0;31m             \u001b[0mjll\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_log_prob_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1462\u001b[0m         \u001b[0mtotal_ll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjll\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_log_prior_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_ll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 6 is out of bounds for axis 1 with size 6"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "model_nb, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18a4c103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 0.107\n",
      "Average bias: 0.105\n",
      "Average variance: 0.035\n",
      "Sklearn 0-1 loss: 0.132\n"
     ]
    }
   ],
   "source": [
    "# Knn\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "knn_vdm, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c0a4f2",
   "metadata": {},
   "source": [
    "## 1.2 EWD, k = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7700c25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype\n",
      "---  ------   --------------  -----\n",
      " 0   slength  150 non-null    int64\n",
      " 1   swidth   150 non-null    int64\n",
      " 2   plength  150 non-null    int64\n",
      " 3   pwidth   150 non-null    int64\n",
      " 4   label    150 non-null    int64\n",
      "dtypes: int64(5)\n",
      "memory usage: 6.0 KB\n",
      "(150, 4) (150,)\n",
      "Class representation - original:  Counter({0: 50, 1: 50, 2: 50})\n",
      "Class representation - training data:  Counter({1: 39, 0: 38, 2: 35})\n",
      "Class representation - testing data:  Counter({2: 15, 0: 12, 1: 11})\n"
     ]
    }
   ],
   "source": [
    "# Complete code for data preperation\n",
    "# Read data\n",
    "df_ewd2 = pd.read_csv('iris_ewd2.csv')\n",
    "disc = 'EWD'\n",
    "k = 7\n",
    "\n",
    "df_ewd2.info()\n",
    "data = df_ewd2.values\n",
    "data.shape\n",
    "\n",
    "features = df_ewd2.drop('label', axis = 1).columns\n",
    "\n",
    "# separate the data into X and y\n",
    "X = data[:, : len(features)]\n",
    "Y = data[:,-1]\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "# Split train test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state = 30)\n",
    "\n",
    "# Check representation of class\n",
    "print('Class representation - original: ', Counter(Y)) \n",
    "print('Class representation - training data: ', Counter(y_train)) \n",
    "print('Class representation - testing data: ', Counter(y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeaca15",
   "metadata": {},
   "source": [
    "### Models - EWD, k=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca83c86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       0.60      0.82      0.69        11\n",
      "           2       0.82      0.60      0.69        15\n",
      "\n",
      "    accuracy                           0.79        38\n",
      "   macro avg       0.81      0.81      0.79        38\n",
      "weighted avg       0.81      0.79      0.79        38\n",
      "\n",
      "Time for training model ID3 - default, EWD, k = 7 is: 0.010634899139404297.\n"
     ]
    }
   ],
   "source": [
    "# ID3 - Default\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "\n",
    "model_id3 = Id3Estimator()\n",
    "model_id3.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_id3 = model_id3.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_id3))\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time for training model ID3 - default, {disc}, k = {k} is: {end - start}.') # Total time execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b220b56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       0.79      1.00      0.88        11\n",
      "           2       1.00      0.80      0.89        15\n",
      "\n",
      "    accuracy                           0.92        38\n",
      "   macro avg       0.93      0.93      0.92        38\n",
      "weighted avg       0.94      0.92      0.92        38\n",
      "\n",
      "Time for training model Naive Bayes - default, EWD, k = 7 is: 0.006596803665161133.\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes - Default\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "model_nb = CategoricalNB()\n",
    "model_nb.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_nb = model_nb.predict(x_test)\n",
    "model_nb.classes_\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "end = time.time()\n",
    "print(f'Time for training model Naive Bayes - default, {disc}, k = {k} is: {end - start}.') # Total time execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0e54ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       1.00      0.91      0.95        11\n",
      "           2       0.94      1.00      0.97        15\n",
      "\n",
      "    accuracy                           0.97        38\n",
      "   macro avg       0.98      0.97      0.97        38\n",
      "weighted avg       0.98      0.97      0.97        38\n",
      "\n",
      "Time for training model Knn-VDM, EWD, k = 7 is: 0.8597049713134766.\n"
     ]
    }
   ],
   "source": [
    "# Knn-VDM complete code\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "\n",
    "# specific the continuous columns index if any\n",
    "vdm = ValueDifferenceMetric(x_train, y_train, continuous = None)\n",
    "vdm.fit()\n",
    "# Knn model, n_neigbour = 3, metrics = vdm\n",
    "knn_vdm = KNeighborsClassifier(n_neighbors=3, metric=vdm.get_distance, algorithm='brute')\n",
    "## Fit model\n",
    "knn_vdm.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_knn = knn_vdm.predict(x_test)\n",
    "knn_vdm.classes_\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time for training model Knn-VDM, {disc}, k = {k} is: {end - start}.') # Total time execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2c56455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation result, accuracy, EWD, k = 7.\n",
      "ID3: - Mean: 0.868889, Standard deviation: 0.105386\n",
      "CNB: - Mean: 0.886667, Standard deviation: 0.092936\n",
      "Knn-VDM: - Mean: 0.935556, Standard deviation: 0.053008\n"
     ]
    }
   ],
   "source": [
    "# CROSS VALIDATION\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# param\n",
    "num_folds = 10\n",
    "num_repeat = 3\n",
    "seed = 7\n",
    "scores = 'accuracy'\n",
    "\n",
    "print(f'Cross validation result, {scores}, {disc}, k = {k}.')\n",
    "\n",
    "# Create list of algorithms\n",
    "models = []\n",
    "models.append(('ID3', Id3Estimator()))\n",
    "#models.append(('RIPPER', lw.RIPPER()))\n",
    "models.append(('CNB', CategoricalNB()))\n",
    "models.append(('Knn-VDM', KNeighborsClassifier(n_neighbors=3, metric=vdm.get_distance, algorithm='brute')))\n",
    "\n",
    "# Evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "  #kfold = KFold(n_splits=num_folds, shuffle = True, random_state=10)\n",
    "    kfold = RepeatedKFold(n_splits=num_folds, n_repeats=num_repeat, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scores)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = '%s: - Mean: %f, Standard deviation: %f' % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed24f03",
   "metadata": {},
   "source": [
    "### Evaluation, EWD, k=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dc8cb65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "#This library is used to decompose bias and variance in our models\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cdbcec02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 0.162\n",
      "Average bias: 0.158\n",
      "Average variance: 0.054\n",
      "Sklearn 0-1 loss: 0.211\n"
     ]
    }
   ],
   "source": [
    "# ID3\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "model_id3, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_id3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6c7ded08",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 6 is out of bounds for axis 1 with size 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-bac3d28e2cd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Naive Bayes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel_nb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'0-1_loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlxtend/evaluate/bias_variance_decomp.py\u001b[0m in \u001b[0;36mbias_variance_decomp\u001b[0;34m(estimator, X_train, y_train, X_test, y_test, loss, num_rounds, random_seed, **fit_params)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_boot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_boot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mall_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mjll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1459\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1461\u001b[0;31m             \u001b[0mjll\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_log_prob_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1462\u001b[0m         \u001b[0mtotal_ll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjll\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_log_prior_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_ll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 6 is out of bounds for axis 1 with size 6"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "model_nb, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "282fdb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 0.048\n",
      "Average bias: 0.026\n",
      "Average variance: 0.040\n",
      "Sklearn 0-1 loss: 0.026\n"
     ]
    }
   ],
   "source": [
    "# Knn\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "knn_vdm, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa5bbd8",
   "metadata": {},
   "source": [
    "## 1.3 EWD, k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b9e17028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype\n",
      "---  ------   --------------  -----\n",
      " 0   slength  150 non-null    int64\n",
      " 1   swidth   150 non-null    int64\n",
      " 2   plength  150 non-null    int64\n",
      " 3   pwidth   150 non-null    int64\n",
      " 4   label    150 non-null    int64\n",
      "dtypes: int64(5)\n",
      "memory usage: 6.0 KB\n",
      "(150, 4) (150,)\n",
      "Class representation - original:  Counter({0: 50, 1: 50, 2: 50})\n",
      "Class representation - training data:  Counter({1: 39, 0: 38, 2: 35})\n",
      "Class representation - testing data:  Counter({2: 15, 0: 12, 1: 11})\n"
     ]
    }
   ],
   "source": [
    "# Complete code for data preperation\n",
    "# Read data\n",
    "df_ewd3 = pd.read_csv('iris_ewd3.csv')\n",
    "disc = 'EWD'\n",
    "k = 10\n",
    "\n",
    "df_ewd3.info()\n",
    "data = df_ewd3.values\n",
    "data.shape\n",
    "\n",
    "features = df_ewd3.drop('label', axis = 1).columns\n",
    "\n",
    "# separate the data into X and y\n",
    "X = data[:, : len(features)]\n",
    "Y = data[:,-1]\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "# Split train test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state = 30)\n",
    "\n",
    "# Check representation of class\n",
    "print('Class representation - original: ', Counter(Y)) \n",
    "print('Class representation - training data: ', Counter(y_train)) \n",
    "print('Class representation - testing data: ', Counter(y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b387e294",
   "metadata": {},
   "source": [
    "### Models, EWD, k=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "baf49400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       0.91      0.91      0.91        11\n",
      "           2       0.93      0.93      0.93        15\n",
      "\n",
      "    accuracy                           0.95        38\n",
      "   macro avg       0.95      0.95      0.95        38\n",
      "weighted avg       0.95      0.95      0.95        38\n",
      "\n",
      "Time for training model ID3 - default, EWD, k = 10 is: 0.01064300537109375.\n"
     ]
    }
   ],
   "source": [
    "# ID3 - Default\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "\n",
    "model_id3 = Id3Estimator()\n",
    "model_id3.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_id3 = model_id3.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_id3))\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time for training model ID3 - default, {disc}, k = {k} is: {end - start}.') # Total time execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a99f37f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       0.91      0.91      0.91        11\n",
      "           2       0.93      0.93      0.93        15\n",
      "\n",
      "    accuracy                           0.95        38\n",
      "   macro avg       0.95      0.95      0.95        38\n",
      "weighted avg       0.95      0.95      0.95        38\n",
      "\n",
      "Time for training model Naive Bayes - default, EWD, k = 10 is: 0.006440877914428711.\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes - Default\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "model_nb = CategoricalNB()\n",
    "model_nb.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_nb = model_nb.predict(x_test)\n",
    "model_nb.classes_\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "end = time.time()\n",
    "print(f'Time for training model Naive Bayes - default, {disc}, k = {k} is: {end - start}.') # Total time execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "35b20a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       0.91      0.91      0.91        11\n",
      "           2       0.93      0.93      0.93        15\n",
      "\n",
      "    accuracy                           0.95        38\n",
      "   macro avg       0.95      0.95      0.95        38\n",
      "weighted avg       0.95      0.95      0.95        38\n",
      "\n",
      "Time for training model Knn-VDM, EWD, k = 10 is: 0.8562920093536377.\n"
     ]
    }
   ],
   "source": [
    "# Knn-VDM complete code\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "\n",
    "# specific the continuous columns index if any\n",
    "vdm = ValueDifferenceMetric(x_train, y_train, continuous = None)\n",
    "vdm.fit()\n",
    "# Knn model, n_neigbour = 3, metrics = vdm\n",
    "knn_vdm = KNeighborsClassifier(n_neighbors=3, metric=vdm.get_distance, algorithm='brute')\n",
    "## Fit model\n",
    "knn_vdm.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_knn = knn_vdm.predict(x_test)\n",
    "knn_vdm.classes_\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time for training model Knn-VDM, {disc}, k = {k} is: {end - start}.') # Total time execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a8596e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation result, accuracy, EWD, k = 10.\n",
      "ID3: - Mean: 0.955556, Standard deviation: 0.055333\n",
      "CNB: - Mean: nan, Standard deviation: nan\n",
      "Knn-VDM: - Mean: 0.966667, Standard deviation: 0.047920\n"
     ]
    }
   ],
   "source": [
    "# CROSS VALIDATION\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# param\n",
    "num_folds = 10\n",
    "num_repeat = 3\n",
    "seed = 7\n",
    "scores = 'accuracy'\n",
    "\n",
    "print(f'Cross validation result, {scores}, {disc}, k = {k}.')\n",
    "\n",
    "# Create list of algorithms\n",
    "models = []\n",
    "models.append(('ID3', Id3Estimator()))\n",
    "#models.append(('RIPPER', lw.RIPPER()))\n",
    "models.append(('CNB', CategoricalNB()))\n",
    "models.append(('Knn-VDM', KNeighborsClassifier(n_neighbors=3, metric=vdm.get_distance, algorithm='brute')))\n",
    "\n",
    "# Evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "  #kfold = KFold(n_splits=num_folds, shuffle = True, random_state=10)\n",
    "    kfold = RepeatedKFold(n_splits=num_folds, n_repeats=num_repeat, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scores)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = '%s: - Mean: %f, Standard deviation: %f' % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c59bdf",
   "metadata": {},
   "source": [
    "### Evaluation, EWD, k=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "26a53d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "#This library is used to decompose bias and variance in our models\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cf65be2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 0.066\n",
      "Average bias: 0.053\n",
      "Average variance: 0.014\n",
      "Sklearn 0-1 loss: 0.053\n"
     ]
    }
   ],
   "source": [
    "# ID3\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "model_id3, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_id3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "12c61717",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 8 is out of bounds for axis 1 with size 8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-bac3d28e2cd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Naive Bayes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel_nb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'0-1_loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlxtend/evaluate/bias_variance_decomp.py\u001b[0m in \u001b[0;36mbias_variance_decomp\u001b[0;34m(estimator, X_train, y_train, X_test, y_test, loss, num_rounds, random_seed, **fit_params)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_boot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_boot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mall_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mjll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1459\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1461\u001b[0;31m             \u001b[0mjll\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_log_prob_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1462\u001b[0m         \u001b[0mtotal_ll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjll\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_log_prior_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_ll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 8 is out of bounds for axis 1 with size 8"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "model_nb, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c430818c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 0.057\n",
      "Average bias: 0.053\n",
      "Average variance: 0.004\n",
      "Sklearn 0-1 loss: 0.053\n"
     ]
    }
   ],
   "source": [
    "# Knn\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "knn_vdm, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc73dbb8",
   "metadata": {},
   "source": [
    "# 2. EFD datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e0ab18",
   "metadata": {},
   "source": [
    "## 2.1 EFD, k = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d9fd9321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype\n",
      "---  ------   --------------  -----\n",
      " 0   slength  150 non-null    int64\n",
      " 1   swidth   150 non-null    int64\n",
      " 2   plength  150 non-null    int64\n",
      " 3   pwidth   150 non-null    int64\n",
      " 4   label    150 non-null    int64\n",
      "dtypes: int64(5)\n",
      "memory usage: 6.0 KB\n",
      "(150, 4) (150,)\n",
      "Class representation - original:  Counter({0: 50, 1: 50, 2: 50})\n",
      "Class representation - training data:  Counter({1: 39, 0: 38, 2: 35})\n",
      "Class representation - testing data:  Counter({2: 15, 0: 12, 1: 11})\n"
     ]
    }
   ],
   "source": [
    "# Complete code for data preperation\n",
    "# Read data\n",
    "df_efd1 = pd.read_csv('iris_efd1.csv')\n",
    "disc = 'EFD'\n",
    "k = 4\n",
    "\n",
    "df_efd1.info()\n",
    "data = df_efd1.values\n",
    "data.shape\n",
    "\n",
    "features = df_efd1.drop('label', axis = 1).columns\n",
    "\n",
    "# separate the data into X and y\n",
    "X = data[:, : len(features)]\n",
    "Y = data[:,-1]\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "# Split train test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state = 30)\n",
    "\n",
    "# Check representation of class\n",
    "print('Class representation - original: ', Counter(Y)) \n",
    "print('Class representation - training data: ', Counter(y_train)) \n",
    "print('Class representation - testing data: ', Counter(y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf93ca5e",
   "metadata": {},
   "source": [
    "### Models, EFD, k=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c53b02a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       0.65      1.00      0.79        11\n",
      "           2       1.00      0.60      0.75        15\n",
      "\n",
      "    accuracy                           0.84        38\n",
      "   macro avg       0.88      0.87      0.85        38\n",
      "weighted avg       0.90      0.84      0.84        38\n",
      "\n",
      "Time for training model ID3 - default, EFD, k = 4 is: 0.009439945220947266.\n"
     ]
    }
   ],
   "source": [
    "# ID3 - Default\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "\n",
    "model_id3 = Id3Estimator()\n",
    "model_id3.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_id3 = model_id3.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_id3))\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time for training model ID3 - default, {disc}, k = {k} is: {end - start}.') # Total time execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "87337b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96        12\n",
      "           1       0.73      1.00      0.85        11\n",
      "           2       1.00      0.80      0.89        15\n",
      "\n",
      "    accuracy                           0.89        38\n",
      "   macro avg       0.91      0.91      0.90        38\n",
      "weighted avg       0.92      0.89      0.90        38\n",
      "\n",
      "Time for training model Naive Bayes - default, EFD, k = 4 is: 0.0066530704498291016.\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes - Default\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "model_nb = CategoricalNB()\n",
    "model_nb.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_nb = model_nb.predict(x_test)\n",
    "model_nb.classes_\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "end = time.time()\n",
    "print(f'Time for training model Naive Bayes - default, {disc}, k = {k} is: {end - start}.') # Total time execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0b4ebdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96        12\n",
      "           1       0.79      1.00      0.88        11\n",
      "           2       1.00      0.87      0.93        15\n",
      "\n",
      "    accuracy                           0.92        38\n",
      "   macro avg       0.93      0.93      0.92        38\n",
      "weighted avg       0.94      0.92      0.92        38\n",
      "\n",
      "Time for training model Knn-VDM, EFD, k = 4 is: 0.8460838794708252.\n"
     ]
    }
   ],
   "source": [
    "# Knn-VDM complete code\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "\n",
    "# specific the continuous columns index if any\n",
    "vdm = ValueDifferenceMetric(x_train, y_train, continuous = None)\n",
    "vdm.fit()\n",
    "# Knn model, n_neigbour = 3, metrics = vdm\n",
    "knn_vdm = KNeighborsClassifier(n_neighbors=3, metric=vdm.get_distance, algorithm='brute')\n",
    "## Fit model\n",
    "knn_vdm.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_knn = knn_vdm.predict(x_test)\n",
    "knn_vdm.classes_\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time for training model Knn-VDM, {disc}, k = {k} is: {end - start}.') # Total time execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6d9633bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation result, accuracy, EFD, k = 4.\n",
      "ID3: - Mean: 0.880000, Standard deviation: 0.094907\n",
      "CNB: - Mean: 0.866667, Standard deviation: 0.091084\n",
      "Knn-VDM: - Mean: 0.926667, Standard deviation: 0.075719\n"
     ]
    }
   ],
   "source": [
    "# CROSS VALIDATION\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# param\n",
    "num_folds = 10\n",
    "num_repeat = 3\n",
    "seed = 7\n",
    "scores = 'accuracy'\n",
    "\n",
    "print(f'Cross validation result, {scores}, {disc}, k = {k}.')\n",
    "\n",
    "# Create list of algorithms\n",
    "models = []\n",
    "models.append(('ID3', Id3Estimator()))\n",
    "#models.append(('RIPPER', lw.RIPPER()))\n",
    "models.append(('CNB', CategoricalNB()))\n",
    "models.append(('Knn-VDM', KNeighborsClassifier(n_neighbors=3, metric=vdm.get_distance, algorithm='brute')))\n",
    "\n",
    "# Evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "  #kfold = KFold(n_splits=num_folds, shuffle = True, random_state=10)\n",
    "    kfold = RepeatedKFold(n_splits=num_folds, n_repeats=num_repeat, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scores)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = '%s: - Mean: %f, Standard deviation: %f' % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3ecbad",
   "metadata": {},
   "source": [
    "### Evaluation, EFD, k=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "65708db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "#This library is used to decompose bias and variance in our models\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c3ec70c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 0.178\n",
      "Average bias: 0.158\n",
      "Average variance: 0.049\n",
      "Sklearn 0-1 loss: 0.158\n"
     ]
    }
   ],
   "source": [
    "# ID3\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "model_id3, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_id3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aef1f055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 0.100\n",
      "Average bias: 0.105\n",
      "Average variance: 0.022\n",
      "Sklearn 0-1 loss: 0.105\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "model_nb, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6b207600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 0.095\n",
      "Average bias: 0.105\n",
      "Average variance: 0.071\n",
      "Sklearn 0-1 loss: 0.079\n"
     ]
    }
   ],
   "source": [
    "# Knn\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "knn_vdm, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927b824b",
   "metadata": {},
   "source": [
    "## 2.2 EFD, k = 7 (iris_efd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c773f70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype\n",
      "---  ------   --------------  -----\n",
      " 0   slength  150 non-null    int64\n",
      " 1   swidth   150 non-null    int64\n",
      " 2   plength  150 non-null    int64\n",
      " 3   pwidth   150 non-null    int64\n",
      " 4   label    150 non-null    int64\n",
      "dtypes: int64(5)\n",
      "memory usage: 6.0 KB\n",
      "(150, 4) (150,)\n",
      "Class representation - original:  Counter({0: 50, 1: 50, 2: 50})\n",
      "Class representation - training data:  Counter({1: 39, 0: 38, 2: 35})\n",
      "Class representation - testing data:  Counter({2: 15, 0: 12, 1: 11})\n"
     ]
    }
   ],
   "source": [
    "# Complete code for data preperation\n",
    "# Read data\n",
    "df_efd2 = pd.read_csv('iris_efd2.csv')\n",
    "disc = 'EFD'\n",
    "k = 7\n",
    "\n",
    "df_efd2.info()\n",
    "data = df_efd2.values\n",
    "data.shape\n",
    "\n",
    "features = df_efd2.drop('label', axis = 1).columns\n",
    "\n",
    "# separate the data into X and y\n",
    "X = data[:, : len(features)]\n",
    "Y = data[:,-1]\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "# Split train test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state = 30)\n",
    "\n",
    "# Check representation of class\n",
    "print('Class representation - original: ', Counter(Y)) \n",
    "print('Class representation - training data: ', Counter(y_train)) \n",
    "print('Class representation - testing data: ', Counter(y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c5d3a1",
   "metadata": {},
   "source": [
    "### Models, EFD, k=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2e8059a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       0.85      1.00      0.92        11\n",
      "           2       1.00      0.87      0.93        15\n",
      "\n",
      "    accuracy                           0.95        38\n",
      "   macro avg       0.95      0.96      0.95        38\n",
      "weighted avg       0.96      0.95      0.95        38\n",
      "\n",
      "Time for training model ID3 - default, EFD, k = 7 is: 0.010675907135009766.\n"
     ]
    }
   ],
   "source": [
    "# ID3 - Default\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "\n",
    "model_id3 = Id3Estimator()\n",
    "model_id3.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_id3 = model_id3.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_id3))\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time for training model ID3 - default, {disc}, k = {k} is: {end - start}.') # Total time execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "265d13fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       0.85      1.00      0.92        11\n",
      "           2       1.00      0.87      0.93        15\n",
      "\n",
      "    accuracy                           0.95        38\n",
      "   macro avg       0.95      0.96      0.95        38\n",
      "weighted avg       0.96      0.95      0.95        38\n",
      "\n",
      "Time for training model Naive Bayes - default, EFD, k = 7 is: 0.007373809814453125.\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes - Default\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "model_nb = CategoricalNB()\n",
    "model_nb.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_nb = model_nb.predict(x_test)\n",
    "model_nb.classes_\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "end = time.time()\n",
    "print(f'Time for training model Naive Bayes - default, {disc}, k = {k} is: {end - start}.') # Total time execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3d456f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       1.00      0.82      0.90        11\n",
      "           2       0.88      1.00      0.94        15\n",
      "\n",
      "    accuracy                           0.95        38\n",
      "   macro avg       0.96      0.94      0.95        38\n",
      "weighted avg       0.95      0.95      0.95        38\n",
      "\n",
      "Time for training model Knn-VDM, EFD, k = 7 is: 0.8499641418457031.\n"
     ]
    }
   ],
   "source": [
    "# Knn-VDM complete code\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "\n",
    "# specific the continuous columns index if any\n",
    "vdm = ValueDifferenceMetric(x_train, y_train, continuous = None)\n",
    "vdm.fit()\n",
    "# Knn model, n_neigbour = 3, metrics = vdm\n",
    "knn_vdm = KNeighborsClassifier(n_neighbors=3, metric=vdm.get_distance, algorithm='brute')\n",
    "## Fit model\n",
    "knn_vdm.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_knn = knn_vdm.predict(x_test)\n",
    "knn_vdm.classes_\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time for training model Knn-VDM, {disc}, k = {k} is: {end - start}.') # Total time execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "22af7e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation result, accuracy, EFD, k = 7.\n",
      "ID3: - Mean: 0.920000, Standard deviation: 0.065320\n",
      "CNB: - Mean: 0.904444, Standard deviation: 0.058836\n",
      "Knn-VDM: - Mean: 0.962222, Standard deviation: 0.047713\n"
     ]
    }
   ],
   "source": [
    "# CROSS VALIDATION\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# param\n",
    "num_folds = 10\n",
    "num_repeat = 3\n",
    "seed = 7\n",
    "scores = 'accuracy'\n",
    "\n",
    "print(f'Cross validation result, {scores}, {disc}, k = {k}.')\n",
    "\n",
    "# Create list of algorithms\n",
    "models = []\n",
    "models.append(('ID3', Id3Estimator()))\n",
    "#models.append(('RIPPER', lw.RIPPER()))\n",
    "models.append(('CNB', CategoricalNB()))\n",
    "models.append(('Knn-VDM', KNeighborsClassifier(n_neighbors=3, metric=vdm.get_distance, algorithm='brute')))\n",
    "\n",
    "# Evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "  #kfold = KFold(n_splits=num_folds, shuffle = True, random_state=10)\n",
    "    kfold = RepeatedKFold(n_splits=num_folds, n_repeats=num_repeat, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scores)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = '%s: - Mean: %f, Standard deviation: %f' % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e9a10c",
   "metadata": {},
   "source": [
    "### Evaluation, EFD, k=7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8de0c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "#This library is used to decompose bias and variance in our models\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "68e3a969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 0.108\n",
      "Average bias: 0.053\n",
      "Average variance: 0.070\n",
      "Sklearn 0-1 loss: 0.053\n"
     ]
    }
   ],
   "source": [
    "# ID3\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "model_id3, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_id3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "25d60b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 0.066\n",
      "Average bias: 0.053\n",
      "Average variance: 0.025\n",
      "Sklearn 0-1 loss: 0.053\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "model_nb, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "07058588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 0.071\n",
      "Average bias: 0.053\n",
      "Average variance: 0.044\n",
      "Sklearn 0-1 loss: 0.053\n"
     ]
    }
   ],
   "source": [
    "# Knn\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "knn_vdm, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98edc4d0",
   "metadata": {},
   "source": [
    "## 2.3 EFD, k =10 (iris_efd3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "060bf707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype\n",
      "---  ------   --------------  -----\n",
      " 0   slength  150 non-null    int64\n",
      " 1   swidth   150 non-null    int64\n",
      " 2   plength  150 non-null    int64\n",
      " 3   pwidth   150 non-null    int64\n",
      " 4   label    150 non-null    int64\n",
      "dtypes: int64(5)\n",
      "memory usage: 6.0 KB\n",
      "(150, 4) (150,)\n",
      "Class representation - original:  Counter({0: 50, 1: 50, 2: 50})\n",
      "Class representation - training data:  Counter({1: 39, 0: 38, 2: 35})\n",
      "Class representation - testing data:  Counter({2: 15, 0: 12, 1: 11})\n"
     ]
    }
   ],
   "source": [
    "# Complete code for data preperation\n",
    "# Read data\n",
    "df_efd3 = pd.read_csv('iris_efd3.csv')\n",
    "disc = 'EFD'\n",
    "k = 10\n",
    "\n",
    "df_efd3.info()\n",
    "data = df_efd3.values\n",
    "data.shape\n",
    "\n",
    "features = df_efd3.drop('label', axis = 1).columns\n",
    "\n",
    "# separate the data into X and y\n",
    "X = data[:, : len(features)]\n",
    "Y = data[:,-1]\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "# Split train test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state = 30)\n",
    "\n",
    "# Check representation of class\n",
    "print('Class representation - original: ', Counter(Y)) \n",
    "print('Class representation - training data: ', Counter(y_train)) \n",
    "print('Class representation - testing data: ', Counter(y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d598463",
   "metadata": {},
   "source": [
    "### Models, EFD, k=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0f0d7849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96        12\n",
      "           1       0.79      1.00      0.88        11\n",
      "           2       1.00      0.87      0.93        15\n",
      "\n",
      "    accuracy                           0.92        38\n",
      "   macro avg       0.93      0.93      0.92        38\n",
      "weighted avg       0.94      0.92      0.92        38\n",
      "\n",
      "Time for training model ID3 - default, EFD, k = 10 is: 0.011312246322631836.\n"
     ]
    }
   ],
   "source": [
    "# ID3 - Default\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "\n",
    "model_id3 = Id3Estimator()\n",
    "model_id3.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_id3 = model_id3.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_id3))\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time for training model ID3 - default, {disc}, k = {k} is: {end - start}.') # Total time execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6973bebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       0.91      0.91      0.91        11\n",
      "           2       0.93      0.93      0.93        15\n",
      "\n",
      "    accuracy                           0.95        38\n",
      "   macro avg       0.95      0.95      0.95        38\n",
      "weighted avg       0.95      0.95      0.95        38\n",
      "\n",
      "Time for training model Naive Bayes - default, EFD, k = 10 is: 0.008018016815185547.\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes - Default\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "model_nb = CategoricalNB()\n",
    "model_nb.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_nb = model_nb.predict(x_test)\n",
    "model_nb.classes_\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "end = time.time()\n",
    "print(f'Time for training model Naive Bayes - default, {disc}, k = {k} is: {end - start}.') # Total time execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c932c97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       0.91      0.91      0.91        11\n",
      "           2       0.93      0.93      0.93        15\n",
      "\n",
      "    accuracy                           0.95        38\n",
      "   macro avg       0.95      0.95      0.95        38\n",
      "weighted avg       0.95      0.95      0.95        38\n",
      "\n",
      "Time for training model Knn-VDM, EFD, k = 10 is: 1.0276598930358887.\n"
     ]
    }
   ],
   "source": [
    "# Knn-VDM complete code\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "\n",
    "# specific the continuous columns index if any\n",
    "vdm = ValueDifferenceMetric(x_train, y_train, continuous = None)\n",
    "vdm.fit()\n",
    "# Knn model, n_neigbour = 3, metrics = vdm\n",
    "knn_vdm = KNeighborsClassifier(n_neighbors=3, metric=vdm.get_distance, algorithm='brute')\n",
    "## Fit model\n",
    "knn_vdm.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_knn = knn_vdm.predict(x_test)\n",
    "knn_vdm.classes_\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time for training model Knn-VDM, {disc}, k = {k} is: {end - start}.') # Total time execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d2d6e746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation result, accuracy, EFD, k = 10.\n",
      "ID3: - Mean: 0.913333, Standard deviation: 0.073333\n",
      "CNB: - Mean: 0.928889, Standard deviation: 0.061904\n",
      "Knn-VDM: - Mean: 0.944444, Standard deviation: 0.054659\n"
     ]
    }
   ],
   "source": [
    "# CROSS VALIDATION\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# param\n",
    "num_folds = 10\n",
    "num_repeat = 3\n",
    "seed = 7\n",
    "scores = 'accuracy'\n",
    "\n",
    "print(f'Cross validation result, {scores}, {disc}, k = {k}.')\n",
    "\n",
    "# Create list of algorithms\n",
    "models = []\n",
    "models.append(('ID3', Id3Estimator()))\n",
    "#models.append(('RIPPER', lw.RIPPER()))\n",
    "models.append(('CNB', CategoricalNB()))\n",
    "models.append(('Knn-VDM', KNeighborsClassifier(n_neighbors=3, metric=vdm.get_distance, algorithm='brute')))\n",
    "\n",
    "# Evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "  #kfold = KFold(n_splits=num_folds, shuffle = True, random_state=10)\n",
    "    kfold = RepeatedKFold(n_splits=num_folds, n_repeats=num_repeat, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scores)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = '%s: - Mean: %f, Standard deviation: %f' % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239d89d7",
   "metadata": {},
   "source": [
    "### Evaluation, EFD, k=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "69df4c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "#This library is used to decompose bias and variance in our models\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f0e4c698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 0.110\n",
      "Average bias: 0.053\n",
      "Average variance: 0.087\n",
      "Sklearn 0-1 loss: 0.079\n"
     ]
    }
   ],
   "source": [
    "# ID3\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "model_id3, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_id3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9765a2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 0.068\n",
      "Average bias: 0.053\n",
      "Average variance: 0.046\n",
      "Sklearn 0-1 loss: 0.053\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "model_nb, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ea12fd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 0.052\n",
      "Average bias: 0.053\n",
      "Average variance: 0.037\n",
      "Sklearn 0-1 loss: 0.053\n"
     ]
    }
   ],
   "source": [
    "# Knn\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "knn_vdm, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e44dc0",
   "metadata": {},
   "source": [
    "# 3. FFD datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c05353",
   "metadata": {},
   "source": [
    "## 3.1 FFD, m =10 (iris_ffd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f526fa2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype\n",
      "---  ------   --------------  -----\n",
      " 0   slength  150 non-null    int64\n",
      " 1   swidth   150 non-null    int64\n",
      " 2   plength  150 non-null    int64\n",
      " 3   pwidth   150 non-null    int64\n",
      " 4   label    150 non-null    int64\n",
      "dtypes: int64(5)\n",
      "memory usage: 6.0 KB\n",
      "(150, 4) (150,)\n",
      "Class representation - original:  Counter({0: 50, 1: 50, 2: 50})\n",
      "Class representation - training data:  Counter({1: 39, 0: 38, 2: 35})\n",
      "Class representation - testing data:  Counter({2: 15, 0: 12, 1: 11})\n"
     ]
    }
   ],
   "source": [
    "# Complete code for data preperation\n",
    "# Read data\n",
    "df_ffd1 = pd.read_csv('iris_ffd1.csv')\n",
    "disc = 'FFD'\n",
    "m = 10\n",
    "\n",
    "df_ffd1.info()\n",
    "data = df_ffd1.values\n",
    "data.shape\n",
    "\n",
    "features = df_ffd1.drop('label', axis = 1).columns\n",
    "\n",
    "# separate the data into X and y\n",
    "X = data[:, : len(features)]\n",
    "Y = data[:,-1]\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "# Split train test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state = 30)\n",
    "\n",
    "# Check representation of class\n",
    "print('Class representation - original: ', Counter(Y)) \n",
    "print('Class representation - training data: ', Counter(y_train)) \n",
    "print('Class representation - testing data: ', Counter(y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492885eb",
   "metadata": {},
   "source": [
    "### Models, FFD, m=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f4485818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       0.91      0.91      0.91        11\n",
      "           2       0.93      0.93      0.93        15\n",
      "\n",
      "    accuracy                           0.95        38\n",
      "   macro avg       0.95      0.95      0.95        38\n",
      "weighted avg       0.95      0.95      0.95        38\n",
      "\n",
      "Time for training model ID3 - default, FFD, m = 10 is: 0.012059926986694336.\n"
     ]
    }
   ],
   "source": [
    "# ID3 - Default\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "\n",
    "model_id3 = Id3Estimator()\n",
    "model_id3.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_id3 = model_id3.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_id3))\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time for training model ID3 - default, {disc}, m = {m} is: {end - start}.') # Total time execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0d80aee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       1.00      0.91      0.95        11\n",
      "           2       0.94      1.00      0.97        15\n",
      "\n",
      "    accuracy                           0.97        38\n",
      "   macro avg       0.98      0.97      0.97        38\n",
      "weighted avg       0.98      0.97      0.97        38\n",
      "\n",
      "Time for training model Naive Bayes - default, FFD, m = 10 is: 0.006140232086181641.\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes - Default\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "model_nb = CategoricalNB()\n",
    "model_nb.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_nb = model_nb.predict(x_test)\n",
    "model_nb.classes_\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "end = time.time()\n",
    "print(f'Time for training model Naive Bayes - default, {disc}, m = {m} is: {end - start}.') # Total time execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e63e5527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       1.00      0.91      0.95        11\n",
      "           2       0.94      1.00      0.97        15\n",
      "\n",
      "    accuracy                           0.97        38\n",
      "   macro avg       0.98      0.97      0.97        38\n",
      "weighted avg       0.98      0.97      0.97        38\n",
      "\n",
      "Time for training model Knn-VDM, FFD, m = 10 is: 0.917590856552124.\n"
     ]
    }
   ],
   "source": [
    "# Knn-VDM complete code\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "\n",
    "# specific the continuous columns index if any\n",
    "vdm = ValueDifferenceMetric(x_train, y_train, continuous = None)\n",
    "vdm.fit()\n",
    "# Knn model, n_neigbour = 3, metrics = vdm\n",
    "knn_vdm = KNeighborsClassifier(n_neighbors=3, metric=vdm.get_distance, algorithm='brute')\n",
    "## Fit model\n",
    "knn_vdm.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_knn = knn_vdm.predict(x_test)\n",
    "knn_vdm.classes_\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time for training model Knn-VDM, {disc}, m = {m} is: {end - start}.') # Total time execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7da8d106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation result, accuracy, FFD, m = 10.\n",
      "ID3: - Mean: 0.948889, Standard deviation: 0.063674\n",
      "CNB: - Mean: 0.935556, Standard deviation: 0.058331\n",
      "Knn-VDM: - Mean: 0.980000, Standard deviation: 0.035066\n"
     ]
    }
   ],
   "source": [
    "# CROSS VALIDATION\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# param\n",
    "num_folds = 10\n",
    "num_repeat = 3\n",
    "seed = 7\n",
    "scores = 'accuracy'\n",
    "\n",
    "print(f'Cross validation result, {scores}, {disc}, m = {m}.')\n",
    "\n",
    "# Create list of algorithms\n",
    "models = []\n",
    "models.append(('ID3', Id3Estimator()))\n",
    "#models.append(('RIPPER', lw.RIPPER()))\n",
    "models.append(('CNB', CategoricalNB()))\n",
    "models.append(('Knn-VDM', KNeighborsClassifier(n_neighbors=3, metric=vdm.get_distance, algorithm='brute')))\n",
    "\n",
    "# Evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "  #kfold = KFold(n_splits=num_folds, shuffle = True, random_state=10)\n",
    "    kfold = RepeatedKFold(n_splits=num_folds, n_repeats=num_repeat, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scores)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = '%s: - Mean: %f, Standard deviation: %f' % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae77296d",
   "metadata": {},
   "source": [
    "### Evaluation, FFD, m=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c20649b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "#This library is used to decompose bias and variance in our models\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "073e2e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 0.084\n",
      "Average bias: 0.053\n",
      "Average variance: 0.032\n",
      "Sklearn 0-1 loss: 0.053\n"
     ]
    }
   ],
   "source": [
    "# ID3\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "model_id3, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_id3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d9133ca1",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 12 is out of bounds for axis 1 with size 12",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-98c83e9ff00f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Naive Bayes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel_nb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'0-1_loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m random_seed=123)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlxtend/evaluate/bias_variance_decomp.py\u001b[0m in \u001b[0;36mbias_variance_decomp\u001b[0;34m(estimator, X_train, y_train, X_test, y_test, loss, num_rounds, random_seed, **fit_params)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_boot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_boot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mall_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mjll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1459\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1461\u001b[0;31m             \u001b[0mjll\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_log_prob_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1462\u001b[0m         \u001b[0mtotal_ll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjll\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_log_prior_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_ll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 12 is out of bounds for axis 1 with size 12"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "model_nb, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e7803404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 0.037\n",
      "Average bias: 0.026\n",
      "Average variance: 0.011\n",
      "Sklearn 0-1 loss: 0.026\n"
     ]
    }
   ],
   "source": [
    "# Knn\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "knn_vdm, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7be02e",
   "metadata": {},
   "source": [
    "## 3.2 FFD, m = 30 (iris_ffd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "668add4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype\n",
      "---  ------   --------------  -----\n",
      " 0   slength  150 non-null    int64\n",
      " 1   swidth   150 non-null    int64\n",
      " 2   plength  150 non-null    int64\n",
      " 3   pwidth   150 non-null    int64\n",
      " 4   label    150 non-null    int64\n",
      "dtypes: int64(5)\n",
      "memory usage: 6.0 KB\n",
      "(150, 4) (150,)\n",
      "Class representation - original:  Counter({0: 50, 1: 50, 2: 50})\n",
      "Class representation - training data:  Counter({1: 39, 0: 38, 2: 35})\n",
      "Class representation - testing data:  Counter({2: 15, 0: 12, 1: 11})\n"
     ]
    }
   ],
   "source": [
    "# Complete code for data preperation\n",
    "# Read data\n",
    "df_ffd2 = pd.read_csv('iris_ffd2.csv')\n",
    "disc = 'FFD'\n",
    "m = 30\n",
    "\n",
    "df_ffd2.info()\n",
    "data = df_ffd2.values\n",
    "data.shape\n",
    "\n",
    "features = df_ffd2.drop('label', axis = 1).columns\n",
    "\n",
    "# separate the data into X and y\n",
    "X = data[:, : len(features)]\n",
    "Y = data[:,-1]\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "# Split train test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state = 30)\n",
    "\n",
    "# Check representation of class\n",
    "print('Class representation - original: ', Counter(Y)) \n",
    "print('Class representation - training data: ', Counter(y_train)) \n",
    "print('Class representation - testing data: ', Counter(y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a3370a",
   "metadata": {},
   "source": [
    "### Models, FFD, m=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "91033ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       0.92      1.00      0.96        11\n",
      "           2       1.00      0.93      0.97        15\n",
      "\n",
      "    accuracy                           0.97        38\n",
      "   macro avg       0.97      0.98      0.97        38\n",
      "weighted avg       0.98      0.97      0.97        38\n",
      "\n",
      "Time for training model ID3 - default, FFD, m = 30 is: 0.013934850692749023.\n"
     ]
    }
   ],
   "source": [
    "# ID3 - Default\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "\n",
    "model_id3 = Id3Estimator()\n",
    "model_id3.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_id3 = model_id3.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_id3))\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time for training model ID3 - default, {disc}, m = {m} is: {end - start}.') # Total time execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "94c7a919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96        12\n",
      "           1       0.90      0.82      0.86        11\n",
      "           2       0.93      0.93      0.93        15\n",
      "\n",
      "    accuracy                           0.92        38\n",
      "   macro avg       0.92      0.92      0.92        38\n",
      "weighted avg       0.92      0.92      0.92        38\n",
      "\n",
      "Time for training model Naive Bayes - default, FFD, m = 30 is: 0.0066907405853271484.\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes - Default\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "model_nb = CategoricalNB()\n",
    "model_nb.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_nb = model_nb.predict(x_test)\n",
    "model_nb.classes_\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "end = time.time()\n",
    "print(f'Time for training model Naive Bayes - default, {disc}, m = {m} is: {end - start}.') # Total time execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fef32cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       0.91      0.91      0.91        11\n",
      "           2       0.93      0.93      0.93        15\n",
      "\n",
      "    accuracy                           0.95        38\n",
      "   macro avg       0.95      0.95      0.95        38\n",
      "weighted avg       0.95      0.95      0.95        38\n",
      "\n",
      "Time for training model Knn-VDM, FFD, m = 30 is: 0.9262831211090088.\n"
     ]
    }
   ],
   "source": [
    "# Knn-VDM complete code\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "\n",
    "# specific the continuous columns index if any\n",
    "vdm = ValueDifferenceMetric(x_train, y_train, continuous = None)\n",
    "vdm.fit()\n",
    "# Knn model, n_neigbour = 3, metrics = vdm\n",
    "knn_vdm = KNeighborsClassifier(n_neighbors=3, metric=vdm.get_distance, algorithm='brute')\n",
    "## Fit model\n",
    "knn_vdm.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_knn = knn_vdm.predict(x_test)\n",
    "knn_vdm.classes_\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time for training model Knn-VDM, {disc}, m = {m} is: {end - start}.') # Total time execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3cbd975a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation result, accuracy, FFD, m = 30.\n",
      "ID3: - Mean: 0.913333, Standard deviation: 0.066999\n",
      "CNB: - Mean: 0.917778, Standard deviation: 0.053564\n",
      "Knn-VDM: - Mean: 0.933333, Standard deviation: 0.068853\n"
     ]
    }
   ],
   "source": [
    "# CROSS VALIDATION\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# param\n",
    "num_folds = 10\n",
    "num_repeat = 3\n",
    "seed = 7\n",
    "scores = 'accuracy'\n",
    "\n",
    "print(f'Cross validation result, {scores}, {disc}, m = {m}.')\n",
    "\n",
    "# Create list of algorithms\n",
    "models = []\n",
    "models.append(('ID3', Id3Estimator()))\n",
    "#models.append(('RIPPER', lw.RIPPER()))\n",
    "models.append(('CNB', CategoricalNB()))\n",
    "models.append(('Knn-VDM', KNeighborsClassifier(n_neighbors=3, metric=vdm.get_distance, algorithm='brute')))\n",
    "\n",
    "# Evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "  #kfold = KFold(n_splits=num_folds, shuffle = True, random_state=10)\n",
    "    kfold = RepeatedKFold(n_splits=num_folds, n_repeats=num_repeat, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scores)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = '%s: - Mean: %f, Standard deviation: %f' % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de967118",
   "metadata": {},
   "source": [
    "### Evaluation, FFD, m=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "73fd3191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "#This library is used to decompose bias and variance in our models\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c83f946e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 0.096\n",
      "Average bias: 0.053\n",
      "Average variance: 0.082\n",
      "Sklearn 0-1 loss: 0.026\n"
     ]
    }
   ],
   "source": [
    "# ID3\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "model_id3, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_id3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "527db7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 0.122\n",
      "Average bias: 0.079\n",
      "Average variance: 0.061\n",
      "Sklearn 0-1 loss: 0.079\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "model_nb, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "15792603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 0.081\n",
      "Average bias: 0.053\n",
      "Average variance: 0.040\n",
      "Sklearn 0-1 loss: 0.053\n"
     ]
    }
   ],
   "source": [
    "# Knn\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "knn_vdm, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645c5260",
   "metadata": {},
   "source": [
    "## 3.3 FFD, m = 60 (iris_ffd3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1c289122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype\n",
      "---  ------   --------------  -----\n",
      " 0   slength  150 non-null    int64\n",
      " 1   swidth   150 non-null    int64\n",
      " 2   plength  150 non-null    int64\n",
      " 3   pwidth   150 non-null    int64\n",
      " 4   label    150 non-null    int64\n",
      "dtypes: int64(5)\n",
      "memory usage: 6.0 KB\n",
      "(150, 4) (150,)\n",
      "Class representation - original:  Counter({0: 50, 1: 50, 2: 50})\n",
      "Class representation - training data:  Counter({1: 39, 0: 38, 2: 35})\n",
      "Class representation - testing data:  Counter({2: 15, 0: 12, 1: 11})\n"
     ]
    }
   ],
   "source": [
    "# Complete code for data preperation\n",
    "# Read data\n",
    "df_ffd3 = pd.read_csv('iris_ffd3.csv')\n",
    "disc = 'FFD'\n",
    "m = 60\n",
    "\n",
    "df_ffd3.info()\n",
    "data = df_ffd3.values\n",
    "data.shape\n",
    "\n",
    "features = df_ffd3.drop('label', axis = 1).columns\n",
    "\n",
    "# separate the data into X and y\n",
    "X = data[:, : len(features)]\n",
    "Y = data[:,-1]\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "# Split train test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state = 30)\n",
    "\n",
    "# Check representation of class\n",
    "print('Class representation - original: ', Counter(Y)) \n",
    "print('Class representation - training data: ', Counter(y_train)) \n",
    "print('Class representation - testing data: ', Counter(y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f07d63",
   "metadata": {},
   "source": [
    "### Models, FFD, m= 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "71836698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86        12\n",
      "           1       0.75      0.82      0.78        11\n",
      "           2       0.88      1.00      0.94        15\n",
      "\n",
      "    accuracy                           0.87        38\n",
      "   macro avg       0.88      0.86      0.86        38\n",
      "weighted avg       0.88      0.87      0.87        38\n",
      "\n",
      "Time for training model ID3 - default, FFD, m = 60 is: 0.010174036026000977.\n"
     ]
    }
   ],
   "source": [
    "# ID3 - Default\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "\n",
    "model_id3 = Id3Estimator()\n",
    "model_id3.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_id3 = model_id3.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_id3))\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time for training model ID3 - default, {disc}, m = {m} is: {end - start}.') # Total time execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "698d4512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80        12\n",
      "           1       0.75      0.27      0.40        11\n",
      "           2       0.88      0.93      0.90        15\n",
      "\n",
      "    accuracy                           0.76        38\n",
      "   macro avg       0.76      0.74      0.70        38\n",
      "weighted avg       0.77      0.76      0.72        38\n",
      "\n",
      "Time for training model Naive Bayes - default, FFD, m = 60 is: 0.008380889892578125.\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes - Default\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "model_nb = CategoricalNB()\n",
    "model_nb.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_nb = model_nb.predict(x_test)\n",
    "model_nb.classes_\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "end = time.time()\n",
    "print(f'Time for training model Naive Bayes - default, {disc}, m = {m} is: {end - start}.') # Total time execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ee4e852f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86        12\n",
      "           1       0.69      0.82      0.75        11\n",
      "           2       0.88      0.93      0.90        15\n",
      "\n",
      "    accuracy                           0.84        38\n",
      "   macro avg       0.86      0.83      0.84        38\n",
      "weighted avg       0.86      0.84      0.84        38\n",
      "\n",
      "Time for training model Knn-VDM, FFD, m = 60 is: 1.1547188758850098.\n"
     ]
    }
   ],
   "source": [
    "# Knn-VDM complete code\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "\n",
    "# specific the continuous columns index if any\n",
    "vdm = ValueDifferenceMetric(x_train, y_train, continuous = None)\n",
    "vdm.fit()\n",
    "# Knn model, n_neigbour = 3, metrics = vdm\n",
    "knn_vdm = KNeighborsClassifier(n_neighbors=3, metric=vdm.get_distance, algorithm='brute')\n",
    "## Fit model\n",
    "knn_vdm.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_knn = knn_vdm.predict(x_test)\n",
    "knn_vdm.classes_\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time for training model Knn-VDM, {disc}, m = {m} is: {end - start}.') # Total time execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f87fc68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation result, accuracy, FFD, m = 60.\n",
      "ID3: - Mean: 0.813333, Standard deviation: 0.103852\n",
      "CNB: - Mean: 0.711111, Standard deviation: 0.093227\n",
      "Knn-VDM: - Mean: 0.528889, Standard deviation: 0.110129\n"
     ]
    }
   ],
   "source": [
    "# CROSS VALIDATION\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# param\n",
    "num_folds = 10\n",
    "num_repeat = 3\n",
    "seed = 7\n",
    "scores = 'accuracy'\n",
    "\n",
    "print(f'Cross validation result, {scores}, {disc}, m = {m}.')\n",
    "\n",
    "# Create list of algorithms\n",
    "models = []\n",
    "models.append(('ID3', Id3Estimator()))\n",
    "#models.append(('RIPPER', lw.RIPPER()))\n",
    "models.append(('CNB', CategoricalNB()))\n",
    "models.append(('Knn-VDM', KNeighborsClassifier(n_neighbors=3, metric=vdm.get_distance, algorithm='brute')))\n",
    "\n",
    "# Evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "  #kfold = KFold(n_splits=num_folds, shuffle = True, random_state=10)\n",
    "    kfold = RepeatedKFold(n_splits=num_folds, n_repeats=num_repeat, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scores)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = '%s: - Mean: %f, Standard deviation: %f' % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3626957",
   "metadata": {},
   "source": [
    "### Evaluation, FFD, m=60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "dc13b12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "#This library is used to decompose bias and variance in our models\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f1a881ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 0.172\n",
      "Average bias: 0.132\n",
      "Average variance: 0.059\n",
      "Sklearn 0-1 loss: 0.132\n"
     ]
    }
   ],
   "source": [
    "# ID3\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "model_id3, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_id3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1b02d26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 0.213\n",
      "Average bias: 0.237\n",
      "Average variance: 0.059\n",
      "Sklearn 0-1 loss: 0.237\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "model_nb, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "92b1e9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 0.260\n",
      "Average bias: 0.132\n",
      "Average variance: 0.189\n",
      "Sklearn 0-1 loss: 0.158\n"
     ]
    }
   ],
   "source": [
    "# Knn\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "knn_vdm, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcae22c3",
   "metadata": {},
   "source": [
    "## 3.4 FFD, m = 100 (iris_ffd4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "05007de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype\n",
      "---  ------   --------------  -----\n",
      " 0   slength  150 non-null    int64\n",
      " 1   swidth   150 non-null    int64\n",
      " 2   plength  150 non-null    int64\n",
      " 3   pwidth   150 non-null    int64\n",
      " 4   label    150 non-null    int64\n",
      "dtypes: int64(5)\n",
      "memory usage: 6.0 KB\n",
      "(150, 4) (150,)\n",
      "Class representation - original:  Counter({0: 50, 1: 50, 2: 50})\n",
      "Class representation - training data:  Counter({1: 39, 0: 38, 2: 35})\n",
      "Class representation - testing data:  Counter({2: 15, 0: 12, 1: 11})\n"
     ]
    }
   ],
   "source": [
    "# Complete code for data preperation\n",
    "# Read data\n",
    "df_ffd4 = pd.read_csv('iris_ffd4.csv')\n",
    "disc = 'FFD'\n",
    "m = 100\n",
    "\n",
    "df_ffd4.info()\n",
    "data = df_ffd4.values\n",
    "data.shape\n",
    "\n",
    "features = df_ffd4.drop('label', axis = 1).columns\n",
    "\n",
    "# separate the data into X and y\n",
    "X = data[:, : len(features)]\n",
    "Y = data[:,-1]\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "# Split train test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state = 30)\n",
    "\n",
    "# Check representation of class\n",
    "print('Class representation - original: ', Counter(Y)) \n",
    "print('Class representation - training data: ', Counter(y_train)) \n",
    "print('Class representation - testing data: ', Counter(y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1e383c",
   "metadata": {},
   "source": [
    "### Models, FFD, m=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c40b8730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86        12\n",
      "           1       0.75      0.82      0.78        11\n",
      "           2       0.88      1.00      0.94        15\n",
      "\n",
      "    accuracy                           0.87        38\n",
      "   macro avg       0.88      0.86      0.86        38\n",
      "weighted avg       0.88      0.87      0.87        38\n",
      "\n",
      "Time for training model ID3 - default, FFD, m = 100 is: 0.01341700553894043.\n"
     ]
    }
   ],
   "source": [
    "# ID3 - Default\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "\n",
    "model_id3 = Id3Estimator()\n",
    "model_id3.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_id3 = model_id3.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_id3))\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time for training model ID3 - default, {disc}, m = {m} is: {end - start}.') # Total time execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b23a35d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80        12\n",
      "           1       0.75      0.27      0.40        11\n",
      "           2       0.88      0.93      0.90        15\n",
      "\n",
      "    accuracy                           0.76        38\n",
      "   macro avg       0.76      0.74      0.70        38\n",
      "weighted avg       0.77      0.76      0.72        38\n",
      "\n",
      "Time for training model Naive Bayes - default, FFD, m = 100 is: 0.008496999740600586.\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes - Default\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "model_nb = CategoricalNB()\n",
    "model_nb.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_nb = model_nb.predict(x_test)\n",
    "model_nb.classes_\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "end = time.time()\n",
    "print(f'Time for training model Naive Bayes - default, {disc}, m = {m} is: {end - start}.') # Total time execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5dd31b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86        12\n",
      "           1       0.69      0.82      0.75        11\n",
      "           2       0.88      0.93      0.90        15\n",
      "\n",
      "    accuracy                           0.84        38\n",
      "   macro avg       0.86      0.83      0.84        38\n",
      "weighted avg       0.86      0.84      0.84        38\n",
      "\n",
      "Time for training model Knn-VDM, FFD, m = 100 is: 1.1199231147766113.\n"
     ]
    }
   ],
   "source": [
    "# Knn-VDM complete code\n",
    "import time\n",
    "start = time.time() # For measuring time execution\n",
    "\n",
    "# specific the continuous columns index if any\n",
    "vdm = ValueDifferenceMetric(x_train, y_train, continuous = None)\n",
    "vdm.fit()\n",
    "# Knn model, n_neigbour = 3, metrics = vdm\n",
    "knn_vdm = KNeighborsClassifier(n_neighbors=3, metric=vdm.get_distance, algorithm='brute')\n",
    "## Fit model\n",
    "knn_vdm.fit(x_train, y_train)\n",
    "# Testing\n",
    "y_pred_knn = knn_vdm.predict(x_test)\n",
    "knn_vdm.classes_\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time for training model Knn-VDM, {disc}, m = {m} is: {end - start}.') # Total time execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2c434b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation result, accuracy, FFD, m = 100.\n",
      "ID3: - Mean: 0.813333, Standard deviation: 0.103852\n",
      "CNB: - Mean: 0.711111, Standard deviation: 0.093227\n",
      "Knn-VDM: - Mean: 0.528889, Standard deviation: 0.110129\n"
     ]
    }
   ],
   "source": [
    "# CROSS VALIDATION\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# param\n",
    "num_folds = 10\n",
    "num_repeat = 3\n",
    "seed = 7\n",
    "scores = 'accuracy'\n",
    "\n",
    "print(f'Cross validation result, {scores}, {disc}, m = {m}.')\n",
    "\n",
    "# Create list of algorithms\n",
    "models = []\n",
    "models.append(('ID3', Id3Estimator()))\n",
    "#models.append(('RIPPER', lw.RIPPER()))\n",
    "models.append(('CNB', CategoricalNB()))\n",
    "models.append(('Knn-VDM', KNeighborsClassifier(n_neighbors=3, metric=vdm.get_distance, algorithm='brute')))\n",
    "\n",
    "# Evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "  #kfold = KFold(n_splits=num_folds, shuffle = True, random_state=10)\n",
    "    kfold = RepeatedKFold(n_splits=num_folds, n_repeats=num_repeat, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scores)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = '%s: - Mean: %f, Standard deviation: %f' % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b0dd85",
   "metadata": {},
   "source": [
    "### Evaluation, FFD, m=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "be76c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "#This library is used to decompose bias and variance in our models\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3544e6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 0.172\n",
      "Average bias: 0.132\n",
      "Average variance: 0.059\n",
      "Sklearn 0-1 loss: 0.132\n"
     ]
    }
   ],
   "source": [
    "# ID3\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "model_id3, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_id3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0a5979e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 0.213\n",
      "Average bias: 0.237\n",
      "Average variance: 0.059\n",
      "Sklearn 0-1 loss: 0.237\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "model_nb, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a6840f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 0.260\n",
      "Average bias: 0.132\n",
      "Average variance: 0.189\n",
      "Sklearn 0-1 loss: 0.158\n"
     ]
    }
   ],
   "source": [
    "# Knn\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "knn_vdm, x_train, y_train, x_test, y_test,\n",
    "loss='0-1_loss',\n",
    "random_seed=123)\n",
    "#---\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f5fd25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
